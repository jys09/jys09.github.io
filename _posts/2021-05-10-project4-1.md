---
layout: post
title: "4차과제 1번"
description: "로지스틱 회귀를 구현하기."
date: 2021-05-11
comments: true
share: true
---

4차 과제1, 조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

--- 
## 1. 데이터 준비하기

### 모듈 불러오기

```
import numpy as np
form sklearn import datasets
import pandas as pd
iris = datasets.load_iris()
```

### 붓꽃 데이터셋의 특성 1가지와 한가지 품종을 선택

```
X = iris["data"][:, 3:]  # 1개의 특성 (꽃잎 너비)
y = (iris["target"] == 2).astype(np.int)   # 버지니카 품종일 때
```

### 모든 샘플에 편향을 추가

```
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

### 결과를 일정하게 유지하기 위한 랜덤 시드를 지정

```
np.random.seed(2042)
```

--- 
## 2. 데이터셋 분할

### 데이터셋 분할 비율 설정

```
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

### 인덱스 무작위로 섞기

```
rnd_indices = np.random.permutation(total_size)
```

### 6:2:2 비율로 훈련, 검증, 테스트 세트를 분할

```
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```

--- 
## 3. 로지스틱 함수 구현

```python
def logistic_sigmoid(x): # 시그모이드 함수 정의
    return 1 / (1 + np.exp(-x))
```

--- 
## 4. 경사하강법 활용 훈련

```
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개
```

### 파라미터를 무작위로 초기 설정

```
Theta = np.random.randn(n_inputs, 1)
```

### 배치경사하강법 구현

```
eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
y_train = y_train.reshape(90,1)

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta)
    Y_proba = logistic_sigmoid(logits)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -1/m*(np.sum(y_train * np.log(Y_proba + epsilon) + (1 - y_train) * np.log(1 - Y_proba + epsilon)))
        print(iteration, loss)

    Y_proba = np.where(Y_proba >= 0.5, 1, 0)
    #print(Y_proba)
    
    error = Y_proba - y_train     # 그레이디언트 계산.
    #print(error)
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```
0 0.7367920641625514   
500 0.6913052093118407   
1000 0.6914566500022249   
1500 0.6913969427170582   
2000 0.6915484030341011   
2500 0.6914886904050195   
3000 0.6914289977295667   
3500 0.6913583974477105   
4000 0.6915098561489456   
4500 0.6914501370427001   
5000 0.6913904378900846   

### 학습된 파라미터

```
Theta
```
###
array([[-0.00836306],
       [ 0.00500979]])

### 배치경사하강법에 대한 정확성 점수 계산

```
logits = X_valid.dot(Theta)              
Y_proba = logistic_sigmoid(logits)
Y_proba_1 = np.where(Y_proba >= 0.5, 1,0)
y_predict = Y_proba_1         # 가장 높은 확률을 갖는 클래스 선택

y_valid = y_valid.reshape(30,1)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```
0.9666666666666667

--- 
## 5. 규제가 추가된 경사하강법 활용 훈련

```
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터
y_train = y_train.reshape(90,1)

Theta = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = logistic_sigmoid(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -1/m*(np.sum(y_train * np.log(Y_proba + epsilon) + (1 - y_train) * np.log(1 - Y_proba + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - y_train
    l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```
0 0.6618882978672164   
500 0.46565264086886404   
1000 0.4649817170971969   
1500 0.4649761853058078   
2000 0.4649761373560999   
2500 0.46497613693929357   
3000 0.46497613693573697   
3500 0.46497613693571294   
4000 0.4649761369357133   
4500 0.46497613693571344   
5000 0.4649761369357135   

### 규제가 추가된 경사하강법에 대한 정확성 점수

```
logits = X_valid.dot(Theta)
Y_proba = logistic_sigmoid(logits)
Y_proba_1 = np.where(Y_proba >= 0.5, 1,0)
y_predict = Y_proba_1         # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```
0.9

--- 
## 6. 조기 종료 추가

```
eta = 0.005
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta)
    Y_proba = logistic_sigmoid(logits)
    error = Y_proba - y_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, 1]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta)
    Y_proba_veri = logistic_sigmoid(logits)
    xentropy_loss = -1/m*(np.sum(y_valid * np.log(Y_proba_veri + epsilon) + (1 - y_valid) * np.log(1 - Y_proba_veri + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```
0 0.3316323837797527   
500 0.20318737444537002   
1000 0.19243960621537481   
1500 0.1880646732824296   
2000 0.1861265275676014   
2395 0.18574640481491134   
2396 0.18574640666650474 조기 종료!

### 조기종료가 추가된 검증세트에 대한 정확도 점수 계산

```
logits = X_valid.dot(Theta)
Y_proba = logistic_sigmoid(logits)
Y_proba_1 = np.where(Y_proba >= 0.5, 1,0)
y_predict = Y_proba_1         # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```
0.9

--- 
## 7. 테스트 세트 평가

```
logits = X_test.dot(Theta)
Y_proba = logistic_sigmoid(logits)
Y_proba_1 = np.where(Y_proba >= 0.5, 1,0)
y_predict = Y_proba_1         # 가장 높은 확률을 갖는 클래스 선택

y_test = y_test.reshape(30,1)

accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```
0.9

--- 

이 문서는 [한글 Lorem Ipsum](http://guny.kr/stuff/klorem/)으로 생성되었습니다.
